{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Analysis of Reinforcement Learning Applied to the 2048 Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Chad Schwenke and Logan Cadman, October 27, 2023* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning concept that mosts interests us is reinforcement learning. Specifically, we think that it would be interesting to analyze how reinforcement learning neural networks can be trained to play games.\n",
    "\n",
    "The game that we will target in this project is 2048. Starting out with a 4x4 game board with the number 2 in a random position, the player must move all tiles up, down, left, or right. Each move will add another 2 or 4 tile in a random empty position, and tiles can only merge and add if they are the same number. The sum of all the tiles is the score and the game ends when the game board is full.\n",
    "\n",
    "We plan to use reinforcement learning to find the \"best\" possible up, left, down, or right player given a particular state of the game board (similar to the storing of tic tac toe game boards seen in class). We think that the A5 Reinforcement Learning assignment can be adapted and applied to this game.\n",
    "\n",
    "After testing and working with the A5 code, we then plan on using libraries (such as Tensorflow or Pytorch) to further increase the capabilities of our game player. We would like to compare and contrast different network structures, optimizers, reward functions, etc. to see which kinds of network can get the highest scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we will adapt the A5 code to be applied to 2048. Afterwards, we will look into Tensorflow or Pytorch implementations of reinforcement learning and apply them to the game as well. The \"data\" will simply be the inputs and outputs to and from the 2048 game.\n",
    "\n",
    "Date | Milestone Title | Milestone Description\n",
    "---|---|---\n",
    "November 3 | Prepare Environment | Create our own implementation of the 2048 game that we can \"attach\" to our reinforcement learning neural network code.\n",
    "November 10 | Adapt A5 Code | Take our implementations of A5 and adapt them to 2048. Write functions to properly \"reward\" the network in the context of 2048. Test various model hyperparameters and see if we can get a network that performs well.\n",
    "November 24 | Tensorflow Implementation | Utilize existing Tensorflow RL libraries to play 2048. Run tests to find the optimal networks using Tensorflow.\n",
    "December 1 | Pytorch Implementation | Utilize existing Pytorch RL libraries to play 2048. Run tests to find the optimal networks using Pytorch.\n",
    "December 8 | Comparison and Analysis | Determine which of the neural network code and methods (A5, Tensorflow, Pytorch) preform best when applied to 2048. Compare which neural network structures, hyperparameters, reward functions, optimizers, etc. score the highest.\n",
    "\n",
    "\n",
    "First, we will work together to develop (or modify an existing implementation, but first will try to do it ourselves) an implementation of the 2048 game that runs in Python. Logan will work on implementing the \"act\" function that will take input from the player and update the game board including moving the tiles, merging and adding tiles of the same number, and creating a new 2 or 4 tile in a random location. Additionally, data structures including the game board and score will be developed by Logan. Chad will work on creating the function(s) that define the reinforcement that the neural network will use as its inputs and rewards. Additionally, Chad will implement any necessary display functions for the game board using exisitng basic graphics libraries.\n",
    "\n",
    "Once we have a working version of 2048 with functions similar to the in class tictactoe example, the next step is to see if we can adapt the code in A5 to train networks to play the game. This step will be done together as our understanding in this area is limited.\n",
    "\n",
    "Next, Chad will work on using Tensorflow to find an optimial implementation of a network that plays 2048. Logan will work on the Pytorch implementation. After work is done on both frameworks seprately, we will join at the end to do a performance comparison and analysis of the two libraries as well as the A5 adaptation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final results will include graphs and tables of metrics pertaining to the games played and the networks playing them. There will also be an explanation of the results and a comparison of what worked best and what did not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect to become very familar with appling reinforcement learning to problems by the end of this project (and the 2048 game itself). The most difficult part will be adapting the A5 code or utilizing the libraries to play the game. We think that the creation of the \"reward function\" will be difficult as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T19:01:45.609927Z",
     "start_time": "2023-10-16T19:01:45.450944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for file ProposalChadLogan.ipynb is 771\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import nbformat\n",
    "import glob\n",
    "nbfile = glob.glob('ProposalChadLogan.ipynb')\n",
    "if len(nbfile) > 1:\n",
    "    print('More than one ipynb file. Using the first one.  nbfile=', nbfile)\n",
    "with io.open(nbfile[0], 'r', encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, nbformat.NO_CONVERT)\n",
    "word_count = 0\n",
    "for cell in nb.cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print('Word count for file', nbfile[0], 'is', word_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
