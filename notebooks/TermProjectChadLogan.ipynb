{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cell with all imports for notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import socket\n",
    "import random\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network to play the 2048 game with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Chad Schwenke and Logan Cadman, December, 2023* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing machine learning our goal was to train a neural network to play the 2048 game. In 2048 you start out with a 4x4 game board with the number 2 in a random position, the player must move all tiles up, down, left, or right. Each move will add another 2 or 4 tile in a random empty position, and tiles can only merge and add if they are the same number. It's a strategic movement game in which you essentially combine tiles to create a higher tile on the game board which increases your score.\n",
    "\n",
    "Training a neural network to play 2048 seemed like an interesting endeavor for multiple reasons. First, our inspiration for this project comes from assignment 5 in class where we trained a reinforcement algorithm to play the tic-tac-toe game. That assignment showed us that training a neural network to play a simple game could be quite interesting. This is because it would allow us to play with different machine learning concepts by providing a practical and fun approach to apple what we've learned in class. \n",
    "\n",
    "2048 is a relatively simple game at first glance, but it is more complex than the tic-tac-toe game. 2048 has clear objectives and easily measurable outcomes such as the score, highest tile, and number of tiles merged. It seems to provide a good platform for experimenting with a multiude of different network structures, learning strategies, optimization methods, and different reward systems. The game requires a strategy and some planning to achieve high scores. Neural networks often use something called a 'discount factore' or in our case we called it 'gamma' this essentially determiens if the agent is going to look for an immediate reward, a future reward, or a balance between the two. This is an interesting system in which the agent must be strategicly planning, and a decreaase in the 'gamma' would show wether or not short term thinking is more or less benefical than an increase in gamma which shows if long term thinking is better. \n",
    "\n",
    "While looking at the tic-tac-toe reinforcement code we realized that it was storing a set of all possible game boards, something that would not be possible in 2048 because there are 18 possible states for each tile, and for a 4x4 board there are 16 total tiles on the board at a time leaving us to believe that there are 18^16 possible board combinations which in no way could be stored in a set. We realized it would however be possible to store a partial set of boards in a set, but did not end up doing this. Rather when the agent plays 2048, they will create there own data and then learn and act upon that data to improve its actions.\n",
    "\n",
    "After doing a bit of work on this project we determined that we would not be adapting the a5 code for the 2048 game. This was due to a combination of factors, including the fact that the a5 reinforcement learning agent was using a set of the possible boards. Overall, we wouldn't have been learning as much if we tried to adapt that code, by creating our own code using PyTorch it allowed us to learn and better understand machine learning. We also didn't want to re-invent the wheel by doing a Tensorflow implementation because it would have been nearly identical, and our goal was to learn RL and not compare frameworks.\n",
    "\n",
    "With so many possible combinations it appears that the maximum tile for 2048 would be either 65,536 OR 131,072. Being that these are so high and neither of us have ever achieved a tile even close to that, it peaked our interest even more. Would it be possible for a neural network agent to achieve a tile with that score? This is an interesting idea because that tile would be extremely hard for a human to achieve.\n",
    "\n",
    "For our neural network we ended up using a few different network designs. These include a simple fully connected network 'QNetworkSimple', another with dropout layers 'QNetworkDropout' but similiar to the simple network, and lastly 'QNetworkConv' a convolutional neural network. We tried a few other designs too, but these are the ones that we did the majority of our tests with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetworkSimple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QNetworkSimple, self).__init__()\n",
    "        self.fc1 = nn.Linear(16, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple network is fully connected and has three layers. It is designed to take in a fully flattened game board with 16 inputs. This is a game board, that has been flattened and from the original 4x4 board. Each layer is represented by fc1, fc2, and fc3. The first layer is taking in 16 inputs and mapping that to 128 outputs, the second layer is taking in 128 and mapping that to 64, and the last layer is taking in 64 and mapping that to 4. The forward function then has 'x = x.to(device)' to ensure we are using a gpu if available. We then activate two of the layers with the relu function so that it is non-linear, and leave the last layer as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetworkDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QNetworkDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(16, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network design is very similiar to our last network but modified to use dropout layers which help with regularization. There are three layers declared, and then one output layer. The dropout function declared after the first two layers is introduced to stop reduce the chance of a value being dropped out. In the forward function we again use relu to add some non-linear and each dropout layer after is used to again add regulurization to the previous layer, this is then repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetworkConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QNetworkConv, self).__init__()\n",
    "        self.conv_block = ConvBlock(input_dim=1, output_dim=32) # Adjust the input dim accordingly\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(4*4*32, 128)  # Adjust the size accordingly\n",
    "        self.fc2 = nn.Linear(128, 4)  # Final layer for 4 decisions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.conv_block(x)\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        d = output_dim // 4\n",
    "        self.conv1 = nn.Conv2d(input_dim, d, kernel_size=1, padding='same')\n",
    "        self.conv2 = nn.Conv2d(input_dim, d, kernel_size=2, padding='same')\n",
    "        self.conv3 = nn.Conv2d(input_dim, d, kernel_size=3, padding='same')\n",
    "        self.conv4 = nn.Conv2d(input_dim, d, kernel_size=4, padding='same')\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape input to [batch_size, channels, height, width]\n",
    "        if len(x.shape) == 1:  # Unbatched input\n",
    "            x = x.view(1, self.input_dim, 4, 4)  # Assuming input is a flattened 4x4 board\n",
    "        elif len(x.shape) == 2:  # Batched input\n",
    "            x = x.view(-1, self.input_dim, 4, 4)  # -1 for batch size\n",
    "\n",
    "        x1 = self.relu(self.conv1(x))\n",
    "        x2 = self.relu(self.conv2(x))\n",
    "        x3 = self.relu(self.conv3(x))\n",
    "        x4 = self.relu(self.conv4(x))\n",
    "        return torch.cat((x1, x2, x3, x4), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code contains two things, first we have a 'QNetworkConv' which is a convulutional neural network, and it required the addition of the second function 'ConvBlock'. This network design has a convolutional block, a flattened layer, and two connected layers. The forward method then passes these layers through the convolutional block, and doing some similiar actions that we have previously discussed for the other designs. The convolutional block used 4 layers as seen by conv1, conv2 and so on. The forward function in the 'ConvBlock' reshapes its input and then applies it to the relu activiation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create this project there were a few things we needed to create. First, we needed a 2048 game board, this was defined as 'class Board', it contains everything related to the game. We then created a class 'replayBuffer' which contains the data that is being created by the agent, that can then be revisited. After that we have our training loop, an evaluation model, and lastly a random evaluation model to test as a baseline. The code and explanations for these are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Board:\n",
    "    def __init__(self):\n",
    "        # Declare variables for score, last added tile position, merges made in last move, and game over flag\n",
    "        self.score = 0\n",
    "        self.last_added_tile = None\n",
    "        self.merges_in_last_move = 0\n",
    "        self.game_over = False\n",
    "\n",
    "        # Initialize board with all zeros (4x4 grid) and then add two tiles\n",
    "        self.board = [[0] * 4 for _ in range(4)]\n",
    "        self._add_new_tile()\n",
    "        self._add_new_tile()\n",
    "\n",
    "    def print_board(self):\n",
    "        # Iterates through each row of the board and prints it\n",
    "        for row in self.board:\n",
    "            print(' '.join(map(str, row)))\n",
    "\n",
    "    def print_score(self):\n",
    "        # Outputs the current score\n",
    "        print(f\"Score: {self.score}\")\n",
    "\n",
    "    def print_highest_tile(self):\n",
    "        # Outputs the current highest tile\n",
    "        print(f\"Highest Tile: {self.highest_tile()}\")\n",
    "\n",
    "    def move_tiles(self, direction):\n",
    "        # Check if game is already over\n",
    "        if self.game_over:\n",
    "            return False\n",
    "\n",
    "        # Reset merge count to zero\n",
    "        self.merges_in_last_move = 0\n",
    "\n",
    "        # Make a copy of the original board\n",
    "        original_board = [row[:] for row in self.board]\n",
    "\n",
    "        # Transpose board for up and down moves\n",
    "        if direction in ('u', 'd'):\n",
    "            self._transpose_board()\n",
    "\n",
    "        # Run logic for all rows or columns in move\n",
    "        for i in range(4):\n",
    "            # Up and left are treated the same after transpose\n",
    "            if direction in ('u', 'l'):\n",
    "                shifted_row = self._shift(self.board[i])\n",
    "                self.board[i] = self._merge(shifted_row)\n",
    "            # Down and right must be reversed first since functions treat everything as left\n",
    "            elif direction in ('d', 'r'):\n",
    "                reversed_row = list(reversed(self.board[i]))\n",
    "                shifted_row = self._shift(reversed_row)\n",
    "                merged_row = self._merge(shifted_row)\n",
    "                # Undo reverse\n",
    "                self.board[i] = list(reversed(merged_row))\n",
    "\n",
    "        # Run transpose again to reset\n",
    "        if direction in ('u', 'd'):\n",
    "            self._transpose_board()\n",
    "\n",
    "        # Check if board changed from original\n",
    "        if self.board != original_board:\n",
    "            # Add a new tile and then check if game is over\n",
    "            self._add_new_tile()\n",
    "            if not self._moves_available():\n",
    "                self.game_over = True\n",
    "            return True\n",
    "        # If board did not change from original, the move was invalid but game is not yet over\n",
    "        return False\n",
    "\n",
    "    def possible_moves(self):\n",
    "        # Create lists to store moves and try all moves\n",
    "        possible_moves = []\n",
    "        directions = ['u', 'd', 'l', 'r']\n",
    "\n",
    "        # Try all possible directions\n",
    "        for direction in directions:\n",
    "            if self._simulate_move(direction):\n",
    "                possible_moves.append(direction)\n",
    "\n",
    "        # Return list of directions\n",
    "        return possible_moves\n",
    "\n",
    "    def get_flattened_board(self):\n",
    "        flattened_board = []\n",
    "        for row in self.board:\n",
    "            for value in row:\n",
    "                # Traverse board and append each value\n",
    "                flattened_board.append(value)\n",
    "        return flattened_board\n",
    "\n",
    "    def get_normalized_flattened_board(self):\n",
    "        normalized_flattened_board = []\n",
    "        for row in self.board:\n",
    "            for value in row:\n",
    "                # Traverse board and append each value normalized to base 2 (except zeros)\n",
    "                normalized_value = math.log(value, 2) if value != 0 else 0\n",
    "                normalized_flattened_board.append(normalized_value)\n",
    "        return normalized_flattened_board\n",
    "\n",
    "    def _transpose_board(self):\n",
    "        # Converts rows to columns and columns to rows\n",
    "        transposed = []\n",
    "        for col_index in range(4):\n",
    "            new_row = []\n",
    "            for row in self.board:\n",
    "                new_row.append(row[col_index])\n",
    "            transposed.append(new_row)\n",
    "        self.board = transposed\n",
    "\n",
    "    def _shift(self, row):\n",
    "        # Shifts non-zero elements to the left in a row\n",
    "        shifted_row = []\n",
    "        for value in row:\n",
    "            if value != 0:\n",
    "                shifted_row.append(value)\n",
    "        # Append zeros to the end of the row to maintain its size\n",
    "        while len(shifted_row) < 4:\n",
    "            shifted_row.append(0)\n",
    "        return shifted_row\n",
    "\n",
    "    def _merge(self, row):\n",
    "        # Merge adjacent tiles with the same value\n",
    "        for i in range(3):\n",
    "            if row[i] == row[i + 1] and row[i] != 0:\n",
    "                row[i] *= 2\n",
    "                row[i + 1] = 0\n",
    "                self.score += row[i]\n",
    "                # Update the number of merges\n",
    "                self.merges_in_last_move += 1\n",
    "        # Shift again to ensure tiles are properly aligned\n",
    "        return self._shift(row)\n",
    "\n",
    "    def _add_new_tile(self):\n",
    "        # Check for possible empty positions\n",
    "        empty_positions = []\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                # If cell is zero it is empty\n",
    "                if self.board[i][j] == 0:\n",
    "                    empty_positions.append((i, j))\n",
    "        if empty_positions:\n",
    "            i, j = random.choice(empty_positions)\n",
    "            # Add a 2 or 4 to a random empty position (10% chance to be 4, 90% chance to be 2)\n",
    "            self.board[i][j] = 4 if random.random() < 0.1 else 2\n",
    "            self.last_added_tile = (i, j)\n",
    "\n",
    "    def _moves_available(self):\n",
    "        # Check if there are any moves available\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                # Check for empty spot\n",
    "                if self.board[i][j] == 0:\n",
    "                    return True\n",
    "                # Check for possible merges in the row\n",
    "                if i < 3 and self.board[i][j] == self.board[i + 1][j]:\n",
    "                    return True\n",
    "                # Check for possible merges in the column\n",
    "                if j < 3 and self.board[i][j] == self.board[i][j + 1]:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def _simulate_move(self, direction):\n",
    "        # Make a copy of the whole object before simulating move\n",
    "        board_copy = copy.deepcopy(self)\n",
    "\n",
    "        # Simulate the move on the copy\n",
    "        board_copy.move_tiles(direction)\n",
    "\n",
    "        # Return weather or not the board changed\n",
    "        return board_copy.board != self.board\n",
    "    \n",
    "    def highest_tile(self):\n",
    "        # Returns the highest tile on the board\n",
    "        highest_tile = 0\n",
    "        for row in self.board:\n",
    "            for value in row:\n",
    "                if value > highest_tile:\n",
    "                    highest_tile = value\n",
    "        return highest_tile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO THINGS ABOUT BOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "# Declare all variables and objects for training\n",
    "q_network = QNetworkSimple().to(device)\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=5e-5)\n",
    "criterion = nn.MSELoss()\n",
    "replay_buffer = ReplayBuffer(capacity=1000)\n",
    "num_episodes = 100\n",
    "batch_size = 100\n",
    "epsilon = 0.9\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = .999\n",
    "action_indices = {'u': 0, 'd': 1, 'l': 2, 'r': 3}\n",
    "gamma = 0.99\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'{socket.gethostname()} has an available cuda GPU.')\n",
    "    \n",
    "if torch.backends.mps.is_available():\n",
    "    print(f'{socket.gethostname()} has an available mps GPU.')\n",
    "\n",
    "# Loop through batches\n",
    "for episode in range(num_episodes):\n",
    "    # Start a new game board and get the initial state\n",
    "    game = Board()\n",
    "    state = game.get_normalized_flattened_board()\n",
    "\n",
    "    # Loop through steps until the game is over\n",
    "    while not game.game_over:\n",
    "        # Get the possible moves\n",
    "        possible_moves = game.possible_moves()\n",
    "\n",
    "        # Choose an action using epsilon-greedy\n",
    "        if np.random.uniform() < epsilon:\n",
    "            with torch.no_grad():\n",
    "                # Get action values from the network\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                action_values = q_network(state_tensor)\n",
    "\n",
    "                # Filter action values for only possible actions\n",
    "                masked_action_values = torch.full(action_values.shape, float('-inf'))\n",
    "                for action in possible_moves:\n",
    "                    index = action_indices[action]\n",
    "                    masked_action_values[0][index] = action_values[0][index]\n",
    "                action = torch.argmax(masked_action_values).item()\n",
    "\n",
    "                # Convert action index to action key\n",
    "                for action_key, index in action_indices.items():\n",
    "                    if index == action:\n",
    "                        action = action_key\n",
    "                        break\n",
    "        # Otherwise choose a random action\n",
    "        else:\n",
    "            action = random.choice(possible_moves)\n",
    "\n",
    "        # Move the tiles using the action and store the reward, state, and done\n",
    "        game.move_tiles(action)\n",
    "        reward = game.merges_in_last_move\n",
    "        next_state = game.get_normalized_flattened_board()\n",
    "        done = game.game_over\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        # Sample and update network if buffer is large enough\n",
    "        if len(replay_buffer.buffer) > batch_size * 10:\n",
    "            batch = replay_buffer.sample(batch_size)\n",
    "            # Split batch into separate components, convert actions to indices\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "            actions_modified = [action_indices[action] for action in actions]\n",
    "\n",
    "            # Convert to tensors\n",
    "            states = torch.FloatTensor(states).to(device)\n",
    "            actions = torch.LongTensor(actions_modified).to(device)\n",
    "            rewards = torch.FloatTensor(rewards).to(device)\n",
    "            next_states = torch.FloatTensor(next_states).to(device)\n",
    "            dones = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "            # Compute current Q values\n",
    "            current_q_values = q_network(states).gather(1, actions.unsqueeze(1))\n",
    "\n",
    "            # Compute next Q values\n",
    "            next_q_values = q_network(next_states).max(1)[0]\n",
    "\n",
    "            # Zero out Q values that will lead to done state\n",
    "            next_q_values[dones] = 0.0\n",
    "\n",
    "            # Compute target Q values\n",
    "            target_q_values = rewards + (gamma * next_q_values)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(current_q_values, target_q_values.unsqueeze(1))\n",
    "\n",
    "            # Optimize the network\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Update the state\n",
    "        state = next_state\n",
    "\n",
    "    # Update epsilon\n",
    "    epsilon *= epsilon_decay\n",
    "\n",
    "    # Display progress\n",
    "    if episode % 100 == 0:\n",
    "            print(f\"Ran {episode} episodes...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO THINGS ABOUT REPLAYBUFFER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(num_games=100):\n",
    "    top_scores = []\n",
    "    total_score = 0\n",
    "    for game_num in range(num_games):\n",
    "        game = Board()\n",
    "        state = game.get_normalized_flattened_board()\n",
    "        while not game.game_over:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                action_values = q_network(state_tensor)              \n",
    "                possible_moves = game.possible_moves()\n",
    "                masked_action_values = torch.full(action_values.shape, float('-inf'))\n",
    "                for move in possible_moves:\n",
    "                    index = action_indices[move]\n",
    "                    masked_action_values[0][index] = action_values[0][index]                \n",
    "                action = torch.argmax(masked_action_values).item()\n",
    "                chosen_action = list(action_indices.keys())[list(action_indices.values()).index(action)]\n",
    "            game.move_tiles(chosen_action)\n",
    "            state = game.get_normalized_flattened_board()\n",
    "        total_score += game.score\n",
    "        top_scores.append(game.highest_tile())\n",
    "        if game_num % 100 == 0:\n",
    "            print(f\"Played {game_num} games...\")\n",
    "    avg_score = total_score / num_games\n",
    "    print(f\"Average Score: {avg_score}\")\n",
    "    return top_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the proposal, summarize what you expect your results to be.  \n",
    "\n",
    "In the final report, show all results.  Intermediate results might be shown in above Methods section.  Plots, tables, whatever is needed to tell your story."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your proposal, describe what you expect to learn and what you expect will be most difficult.\n",
    "\n",
    "In your project report, describe what you learned, and what was most difficult.  Summarize any surprises you encontered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Goodfellow, et al., 2016] Ian Goodfellow and Yoshua Bengio and Aaron Courville, [Deep Learning](http://www.deeplearningbook.org), MIT Press. 2014.\n",
    "* [Baluja, 2021] Michael Baluja, [Reinforcement Learning for 2048](https://github.com/michaelbaluja/rl-2048), 2021\n",
    "* [Virdee, 2018] Navjinder Virdee [Trained A Neural Network To Play 2048 using Deep-Reinforcement Learning](https://github.com/navjindervirdee/2048-deep-reinforcement-learning), 2018\n",
    "* [Pan, 2019] Tianyi Pan [Applied Reinforcement Learning with 2048](https://www.linkedin.com/pulse/part-1-applied-reinforcement-learning-2048-tianyi-pan), 2019\n",
    "* [Goenawan, 2020] Nathaniel Goenawan and Simon Tao and Katherine Wu [What’s in a Game: Solving 2048 with Reinforcement Learning](https://web.stanford.edu/class/aa228/reports/2020/final41.pdf), Stanford 2020\n",
    "* [Paszke, 2023] Adam Paszke and Mark Towers [Reinforcement Learning (DQN) Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html), PyTorch 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T19:01:45.609927Z",
     "start_time": "2023-10-16T19:01:45.450944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for file TermProjectChadLogan.ipynb is 1299\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import nbformat\n",
    "import glob\n",
    "nbfile = glob.glob('TermProjectChadLogan.ipynb')\n",
    "if len(nbfile) > 1:\n",
    "    print('More than one ipynb file. Using the first one.  nbfile=', nbfile)\n",
    "with io.open(nbfile[0], 'r', encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, nbformat.NO_CONVERT)\n",
    "word_count = 0\n",
    "for cell in nb.cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print('Word count for file', nbfile[0], 'is', word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
